#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language spanish
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Índice
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
\begin_inset VSpace vfill
\end_inset


\family sans
\series bold
\size giant
REDES NEURONALES
\begin_inset Newline newline
\end_inset

Practico 3
\begin_inset Newline newline
\end_inset


\family default
\series default
Red Perceptron
\size default

\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Author
Prof: F.
 A.
 Tamarit
\begin_inset Newline newline
\end_inset

Autor: Eric N.
 Jurio
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Part*
Introducción
\end_layout

\begin_layout Section*
Características del modelo de Hopfield diluido
\end_layout

\begin_layout Standard
En términos generales, el modelo es el mismo al del Hopfield tradicional
 salvo por algunas diferencias.
 Aquí presentamos una síntesis de las diferencias del modelo de Hopfield
 diluido con el modelo tradicional.
\end_layout

\begin_layout Itemize
A diferencia del modelo de Hopfield tradicional, en este modelo, las neuronas
 tienen un grado de conectividad mucho menor a la cantidad de neuronas y
 además es variante (para nuestro caso, cada neurona se conecta con otras
 20 y el grado de conectividad es igual para todas las neuronas).
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
N\text{°}vecinos\ll N\text{°}neuronas
\]

\end_inset


\end_layout

\begin_layout Itemize
Otra diferencia es que, la matriz de pesos en el conexionado de las neuronas
 (
\begin_inset Formula $\omega$
\end_inset

) no es simétrica, lo cual es mas realista desde el punto de vista biológico.
\end_layout

\begin_layout Itemize
Este tipo de redes, al igual que las redes de Hopfield tradicionales, son
 mayormente usado como memoria asociativa.
 Donde para agregar un concepto 
\begin_inset Formula $\xi^{\mu}$
\end_inset

 (representado normalmente por un arreglo de bits) a la matriz de pesos
 
\begin_inset Formula $\omega$
\end_inset

 se realiza la siguiente operación
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
\omega_{i,j}^{new}=\omega_{i,j}^{old}+(\eta\cdot\xi_{i}^{\mu}\cdot\xi_{j}^{\mu})
\]

\end_inset


\end_layout

\begin_layout Itemize
Para el modelo tradicional de Hopfield, al ser inicializada cerca de un
 concepto, la red reconoce bien si se cumple:
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
\frac{N\text{°}conceptos}{N\text{°}neuronas}\leq0.138
\]

\end_inset

En cambio, en el modelo de Hopfield diluido, al ser inicializada cerca de
 un concepto, lo importante no es la relación 
\begin_inset Formula $\frac{N\text{°}conceptos}{N\text{°}neuronas}$
\end_inset

, sino la relación 
\begin_inset Formula $\frac{N\text{°}conceptos}{N\text{°}conexionesPromedio}$
\end_inset

.
 El grado de reconocimiento de la red decae paulatinamente y su caída abrupta
 esta cerca de:
\begin_inset Newline newline
\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
Recordar 
\begin_inset Formula $\frac{2}{\pi}\backsimeq0.6366$
\end_inset


\end_layout

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
\frac{N\text{°}conceptos}{N\text{°}conexionesPromedio}\leq\frac{2}{\pi}
\]

\end_inset


\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section*
Objetivos
\end_layout

\begin_layout Standard
En este informe analizaremos el grado de reconocimiento de las redes neuronales
 de Hopfield diluido, para distintas cantidades de neuronas y conceptos.
\end_layout

\begin_layout Standard
Llamaremos <m>
\begin_inset Foot
status open

\begin_layout Plain Layout
<m> = Grado de reconocimiento de la red.
\end_layout

\end_inset

 al promedio de bits acertados respecto del concepto patrón al estabilizarse
 la red a lo largo del tiempo, siendo inicializada cerca (dentro de la cuenca
 de atracción) del concepto patrón (mediremos el porcentaje de reconocimiento
 o porcentaje de bits que coinciden entre el concepto patrón y el resultado
 de la red).
\end_layout

\begin_layout Standard
Analizaremos el <m> respecto de la relación 
\begin_inset Formula $\frac{N\text{°}conceptos}{N\text{°}conexionesPromedio}$
\end_inset

 (para nuestro caso el 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
N°conexiones Promedio
\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 es constante, igual a 20) fijando la cantidad de neuronas y variando la
 cantidad de conceptos, a su vez tomando el promedio de 30 corridas distintas
 para cada cantidad de conceptos.
 Este procedimiento lo realizaremos para tres tamaños distintos de redes.
\end_layout

\begin_layout Section*
Detalles de la implementación
\end_layout

\begin_layout Standard
En la implementación del modelo de Hopfield diluido, a diferencia de la
 implementación de Hopfield tradicional, donde utilizamos varias optimizaciones,
 aquí no se puede aplicar optimizaciones y no son necesarias, ya que por
 ser una red diluida la cantidad de interacciones con los vecinos son pocas.
\end_layout

\begin_layout Standard
Aquí si usamos la matriz de pesos W (la cual, en este caso, no es simétrica).
 También se aplico la optimización a nivel de bits lo cual disminuye el
 uso de memoria.
\end_layout

\begin_layout Part*
Red de Hopfield Diluidas
\end_layout

\begin_layout Standard
Realizamos experimentos con redes de Hopfield diluidas para N={2500;5000;10000}
 neuronas y P conceptos variando de 1 a 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
N°conexiones.
 
\begin_inset Foot
status open

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
N°conexiones
\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 = 20
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A cada paso tomamos el promedio de 30 mediciones del grado de reconocimiento,
 para distintos conceptos aleatorios y graficamos el <m> respecto de 
\begin_inset Formula $\frac{N\text{°}conceptos}{N\text{°}conexiones}$
\end_inset

, para cada tamaño de red (diferentes N), obteniendo como resultados gráficos
 muy similares.
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
A continuación observamos el gráfico para 10000 neuronas con barras de error.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename img/ejer1_EB_N10000.jpg
	scale 50
	rotateOrigin center

\end_inset


\shape italic
\size tiny

\begin_inset Caption

\begin_layout Plain Layout
<m> respecto de
\begin_inset Formula $\frac{N\text{°}conceptos}{N\text{°}conexiones}$
\end_inset

 con barras de error.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Se puede ver claramente que la pendiente de la curva es paulatinamente decrecien
te, dándole al gráfico una forma semicircular para 
\begin_inset Formula $\frac{N\text{°}conceptos}{N\text{°}conexiones}\leq0.55$
\end_inset

.
\end_layout

\begin_layout Itemize
Podemos ver claramente como la red tiene un <m> de aproximadamente 1 y su
 desviación estándar es 0 cuando se da que 
\begin_inset Formula $\frac{N\text{°}conceptos}{N\text{°}conexiones}\leq0.3$
\end_inset

 indicando que la red se va al atractor o muy cerca de el.
\end_layout

\begin_layout Itemize
Cuando 
\begin_inset Formula $0.3\leq\frac{N\text{°}conceptos}{N\text{°}conexiones}\leq0.45$
\end_inset

 el <m> decrece paulatinamente de 0.93 a 0.73 y su desviación estándar es
 aun mínima.
 La red tiende a desplazarse del atractor quedando en algún radio dentro
 de la cuenca de atracción pero cada vez mas alejado a medida que crece
 la cantidad de conceptos.
\end_layout

\begin_layout Itemize
Cuando 
\begin_inset Formula $0.45\leq\frac{N\text{°}conceptos}{N\text{°}conexiones}\leq0.6$
\end_inset

 el <m> decrece abruptamente de 0.73 a 0.03 y su desviación estándar es máxima,
 marcando la caída pronunciada de la curva.
 La red tiende a desplazarse aun mas del atractor, aveces reconoce (quedándose
 cerca del atractor) y otras veces no reconoce (siendo atrapada en otra
 cuenca de otro atractor).
\end_layout

\begin_layout Itemize
Para 
\begin_inset Formula $0.6\leq\frac{N\text{°}conceptos}{N\text{°}conexiones}$
\end_inset

 se puede observar que <m> se mantiene casi constante en el intervalo 
\begin_inset Formula $\left[0.053;-0.005\right]$
\end_inset

 decreciendo paulatinamente con una desviación estándar de 0.08 a 0.02 lo
 que indica que la red, prácticamente no reconoce y se mantiene lejos del
 patrón inicial, quizás en la cuenca de otro atractor vecino.
 Es decir para 
\begin_inset Formula $0.6\leq\frac{N\text{°}conceptos}{N\text{°}conexiones}$
\end_inset

 la red tiende a salir de su cuenca de atracción y ser atraída por otros
 atractores.
\end_layout

\begin_layout Standard
A continuación comparamos los tres gráficos superpuestos sin las barras
 de error.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename img/ejer1.jpg
	scale 50
	rotateOrigin center

\end_inset


\end_layout

\begin_layout Plain Layout

\shape italic
\size tiny
\begin_inset Caption

\begin_layout Plain Layout
3 curvas <m> respecto de 
\begin_inset Formula $\frac{N\text{°}conceptos}{N\text{°}conexiones}$
\end_inset

 para distintos N° de neuronas.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Como se dijo al principio, las 3 curvas son similares y a medida que se
 incrementa el numero de neuronas en la red, se obtienen resultados mas
 nítidos en las curvas.
\end_layout

\begin_layout Itemize
Notamos como, manteniendo la relación 
\begin_inset Formula $\frac{N\text{°}conceptos}{N\text{°}conexiones}\leq0.4$
\end_inset

, el grado de reconocimiento es mayor al 0.8 independientemente del numero
 de neuronas.
\end_layout

\begin_layout Itemize
Podemos ver también que cuando 
\begin_inset Formula $0.4\leq\frac{N\text{°}conceptos}{N\text{°}conexiones}\leq0.65$
\end_inset

, la pendiente de las curvas es cada vez mas pronunciada a medida que se
 aumenta el numero de neuronas.
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
En el siguiente gráfico veremos las 3 curvas, agregando barras de error.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename img/ejer1_EB.jpg
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout

\shape slanted
\size tiny
\begin_inset Caption

\begin_layout Plain Layout
Ídem Figura 0.2, con barras de error.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Aquí observamos que a medida que aumenta el numero de neuronas, la máxima
 desviación estándar se registra para 
\begin_inset Formula $\frac{N\text{°}conceptos}{N\text{°}conexiones}$
\end_inset

 cada vez mayores.
 Lo que nos evidencia, que en situaciones ideales
\begin_inset Foot
status open

\begin_layout Plain Layout
Es decir 
\begin_inset Formula $N\text{°}conexiones\ll N\text{°}neuronas$
\end_inset


\end_layout

\end_inset

, el grado de reconocimiento de la red decae paulatinamente y su caída abrupta
 esta cerca de:
\begin_inset Newline newline
\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
Recordar 
\begin_inset Formula $\frac{2}{\pi}\backsimeq0.6366$
\end_inset


\end_layout

\end_inset


\begin_inset Formula 
\[
\frac{N\text{°}conceptos}{N\text{°}conexionesPromedio}\leq\frac{2}{\pi}
\]

\end_inset


\end_layout

\begin_layout Part*
\begin_inset Newpage pagebreak
\end_inset

Conclusiones
\end_layout

\begin_layout Subsection*
Redes de Hopfield Diluidas
\end_layout

\begin_layout Enumerate
Pudimos ver claramente en los gráficos de las redes de Hopfield diluidas,
 que cuando 
\begin_inset Formula $0.45\leq\frac{N\text{°}conceptos}{N\text{°}conexiones}$
\end_inset

, la red deja de reconocer.
\end_layout

\begin_layout Enumerate
A medida que incrementamos el numero de neuronas en la red, los gráficos
 resultantes se asemejan a los vistos en la teoría.
\end_layout

\begin_layout Enumerate
Notamos en los gráficos, que el limite de reconocimiento observado de la
 red esta cerca de 0.55 lo cual esta por debajo de lo visto en la teoría.
\begin_inset Formula 
\[
\frac{N\text{°}conceptos}{N\text{°}conexionesPromedio}\leq\frac{2}{\pi}
\]

\end_inset

Suponemos esto se debe a que no respetamos estrictamente el modelo de Hopfield
 diluido, ya que en el experimento, el numero de conexiones de cada neurona
 es constante K igual para todas las neuronas y no, como especifica el modelo,
 numero de conexiones aleatorio con media K.
\end_layout

\end_body
\end_document
