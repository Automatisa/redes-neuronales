#LyX 1.6.7 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
\begin_inset VSpace vfill
\end_inset


\family sans
\series bold
\size giant
REDES NEURONALES
\begin_inset Newline newline
\end_inset

Practico 1
\begin_inset Newline newline
\end_inset


\family default
\series default
Redes de Hopfield
\size default

\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Author
Prof: F.
 A.
 Tamarit
\begin_inset Newline newline
\end_inset

Autor: Eric N.
 Jurio
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Part*
Introducción
\end_layout

\begin_layout Section*
Características del modelo de Hopfield
\end_layout

\begin_layout Standard
Aquí presentamos un resumen de las características del modelo de Hopfield
 según lo visto en el teórico.
\end_layout

\begin_layout Itemize
En este modelo en particular todas las neuronas están conectadas a todas.
\end_layout

\begin_layout Itemize
La matriz de pesos en el conexionado de las neuronas (W) es simétrica, lo
 cual es irreal desde el punto de vista biológico, pero facilita mucho las
 cosas a la hora de hacer cuentas sobre el modelo.
\end_layout

\begin_layout Itemize
Este tipo de redes es mayormente usado como memoria asociativa.
\begin_inset Newline newline
\end_inset

Donde para agregar un concepto 
\begin_inset Formula $\text{e}^{\text{u}}$
\end_inset

 (representado normalmente por un arreglo de bits) a la matriz de pesos
 W se realiza la siguiente operación
\begin_inset Newline newline
\end_inset


\begin_inset Formula \[
W_{i,j}=\text{e}_{i}^{\text{u}}\cdot\text{e}_{j}^{\text{u}}\]

\end_inset


\end_layout

\begin_layout Itemize
Para que la red funcione bien, los conceptos tienen que ser notablemente
 distintos.
 Tienen que estar distribuidos aleatoriamente en el espacio de configuraciones.
\end_layout

\begin_layout Itemize
Los atractores son puntos fijos, que en el caso determinista, la red se
 estabiliza y en caso de neuronas estocásticas la red queda oscilando alrededor
 del atractor.
\end_layout

\begin_layout Itemize
Para el modelo tradicional de red neuronal de Hopfield, al ser inicializada
 cerca de un concepto, la red reconoce bien si se cumple
\begin_inset Newline newline
\end_inset


\begin_inset Formula \[
\frac{N\text{º}conceptos}{N\text{º}neuronas}\leq0.138\]

\end_inset


\end_layout

\begin_layout Itemize
En el caso de neuronas estocásticas, dependiendo del nivel de ruido, pueden
 saltar barreras o rugosidades en el espacio de configuraciones llegando
 a mínimos mas profundos, pero a medida que la relación 
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}$
\end_inset

 decrece, el nivel ruido al cual la red deja de reconocer también decrece.
\end_layout

\begin_layout Standard
Estos últimos puntos serán observados en los gráficos resultados de los
 experimentos.
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section*
Objetivos
\end_layout

\begin_layout Standard
En este informe analizaremos el grado de reconocimiento de las redes neuronales
 de Hopfield usando tanto neuronas deterministas como estocásticas, para
 distintas cantidades de neuronas y conceptos (variando el nivel de ruido
 en el caso estocástico).
\end_layout

\begin_layout Standard
Llamaremos grado de reconocimiento de la red (<m>) al promedio de bits acertados
 respecto del concepto patrón al estabilizarse la red a lo largo del tiempo
 (en el caso determinista) o al transcurrir una cierta cantidad de ciclos
 (en el caso estocástico), siendo inicializada cerca (dentro de la cuenca
 de atracción) del concepto patrón (mediremos el porcentaje de reconocimiento
 o porcentaje de bits que coinciden entre el concepto patrón y el resultado
 de la red)
\end_layout

\begin_layout Standard
En el caso determinista, analizaremos el grado de reconocimiento de la red,
 respecto de la relación 
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}$
\end_inset

, fijando la cantidad de neuronas y variando la cantidad de conceptos, a
 su vez tomando el promedio de 30 corridas distintas para cada cantidad
 de conceptos.
 Este procedimiento lo realizaremos para tres tamaños distintos de redes.
\end_layout

\begin_layout Standard
En el caso estocástico, analizaremos el grado de reconocimiento de la red,
 a medida que incrementamos el nivel de ruido.
 Esto lo haremos para 3 tamaños de redes y para 3 cantidades de conceptos
 distintos por tamaño, comparando los resultados para experimentos que tengan
 una misma relación 
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}$
\end_inset

.
\end_layout

\begin_layout Section*
Detalles de la implementación
\end_layout

\begin_layout Standard
En la implementación del modelo de Hopfield usada para la experiencia, utilizamo
s todas las optimizaciones vistas en el teórico.
 Por lo tanto no usamos la matriz de pesos W.
 También se aplico la optimización a nivel de bits lo cual disminuye el
 uso de memoria.
\end_layout

\begin_layout Standard
En pruebas adicionales notamos la marcada diferencia en tiempo de procesamiento
 para misma cantidades de conceptos y distintos números de neuronas.
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Part*
Red de Hopfield Determinista
\end_layout

\begin_layout Standard
Realizamos experimentos con redes neuronales de Hopfield para N neuronas
 y P conceptos con las siguientes relaciones:
\end_layout

\begin_layout Itemize
Para N=416 y 
\begin_inset Formula $P\in\left[2;208\right]$
\end_inset

 con paso de a 2.
\end_layout

\begin_layout Itemize
Para N=832 y 
\begin_inset Formula $P\in\left[4;416\right]$
\end_inset

 con paso de a 4.
\end_layout

\begin_layout Itemize
Para N=1664 y 
\begin_inset Formula $P\in\left[8;832\right]$
\end_inset

 con paso de a 8.
\end_layout

\begin_layout Standard
A cada paso tomamos el promedio de 30 mediciones del grado de reconocimiento,
 para distintos conceptos aleatorios y graficamos el grado de reconocimiento
 (<m>) respecto de 
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}$
\end_inset

 obteniendo como resultados gráficos muy similares.
\end_layout

\begin_layout Standard
A continuación observamos el gráfico para 1664 neuronas, con P variando
 de 8 a 832 con barras de error.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename img/ejer1_EB_N1664.jpg
	scale 50
	rotateOrigin center

\end_inset


\shape italic
\size tiny

\begin_inset Caption

\begin_layout Plain Layout
<m> (grado de reconocimiento) respecto de 
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}$
\end_inset

 con barras de error.
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Itemize
Podemos ver claramente como la red tiene un grado de reconocimiento (<m>)
 de 1 y su desviación estándar es 0 cuando se da que 
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}\leq0.13$
\end_inset

 indicando que la red se va al atractor o muy cerca de el.
\end_layout

\begin_layout Itemize
Cuando 
\begin_inset Formula $0.13\leq\frac{N\text{º}conceptos}{N\text{º}neuronas}\leq0.17$
\end_inset

 el <m> decrece abruptamente de 1 a 0.3 y su desviación estándar es máxima
 indicando una etapa de transición pronunciada.
 La red tiende a desplazarse del atractor quedando en algún radio dentro
 de la cuenca de atracción pero cada vez mas alejado a medida que crece
 la cantidad de conceptos; o bien aveces reconoce (quedándose cerca del
 atractor) y otras veces no reconoce (siendo atrapada en otra cuenca de
 otro atractor).
\end_layout

\begin_layout Itemize
Para 
\begin_inset Formula $0.17\leq\frac{N\text{º}conceptos}{N\text{º}neuronas}$
\end_inset

 se puede observar que <m> se mantiene casi constante en el intervalo 
\begin_inset Formula $\left[0.3;0.25\right]$
\end_inset

 decreciendo paulatinamente con una desviación estándar de 0.1 lo que indica
 que la red, prácticamente no reconoce y se mantiene lejos del patrón inicial,
 quizás en la cuenca de otro atractor vecino.
 Es decir para 
\begin_inset Formula $0.17\leq\frac{N\text{º}conceptos}{N\text{º}neuronas}$
\end_inset

 la red tiende a salir de su cuenca de atracción y ser atraída por otros
 atractores.
\end_layout

\begin_layout Standard
A continuación comparamos los tres gráficos superpuestos sin las barras
 de error.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename img/ejer1.jpg
	scale 50
	rotateOrigin center

\end_inset


\end_layout

\begin_layout Plain Layout

\shape italic
\size tiny
\begin_inset Caption

\begin_layout Plain Layout
3 curvas <m> (grado de reconocimiento) respecto de 
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}$
\end_inset

 para distintas cantidades de neuronas.
 
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Itemize
Como se dijo al principio, las 3 curvas son similares y a medida que se
 incrementa el numero de neuronas en la red, se obtienen resultados mas
 nítidos en las curvas.
\end_layout

\begin_layout Itemize
Notamos como, manteniendo la relación 
\begin_inset Formula $0.17\leq\frac{N\text{º}conceptos}{N\text{º}neuronas}$
\end_inset

, el grado de reconocimiento toma valores cada vez menores a medida que
 se incrementa el numero de neuronas, lo cual se condice con la teoría que
 dice que para redes de gran cantidad de neurona, cuando 
\begin_inset Formula $0.138\leq\frac{N\text{º}conceptos}{N\text{º}neuronas}$
\end_inset

, el grado de reconocimiento tiende a 0.
\end_layout

\begin_layout Itemize
Podemos ver también que cuando 
\begin_inset Formula $0.13\leq\frac{N\text{º}conceptos}{N\text{º}neuronas}\leq0.17$
\end_inset

, la pendiente de las curvas es cada vez mas pronunciada a medida que se
 aumenta el numero de neuronas.
\end_layout

\begin_layout Part*
Red de Hopfield Estocástica
\end_layout

\begin_layout Standard
Para cada experimento con redes neuronales de Hopfield estocásticas, fijamos
 la cantidad de neuronas y de conceptos.
 Realizamos las distintas mediciones de grado de reconocimiento, variando
 el nivel de ruido de las neuronas de la red en el rango 
\begin_inset Formula $\left[0.1;1.2\right]$
\end_inset

.
 Agrupamos las curvas resultantes según conservan la misma relación 
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}$
\end_inset

 de la siguiente manera:
\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}=0.012$
\end_inset

 Con N=416; 832; 1664 y P=5;10;20 respectivamente.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}=0.024$
\end_inset

 Con N=416; 832; 1664 y P=10;20;40 respectivamente.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}=0.036$
\end_inset

 Con N=416; 832; 1664 y P=15;30;60 respectivamente.
\end_layout

\begin_layout Standard
Para cada nivel de ruido tomamos el promedio de 30 mediciones del grado
 de reconocimiento, para distintos conceptos aleatorios y graficamos el
 grado de reconocimiento (<m>) respecto del nivel de ruido, obteniendo como
 resultados gráficos muy similares.
 (Para cada medición dejamos evolucionar previamente algunos ciclos a la
 red)
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename img/ejer2_012.jpg
	scale 50
	rotateOrigin center

\end_inset


\end_layout

\begin_layout Plain Layout

\shape italic
\size tiny
\begin_inset Caption

\begin_layout Plain Layout
Grado de reconocimiento <m> respecto del nivel de ruido para la relación
 
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}=0.012$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Podemos observar que a partir de un nivel de ruido de 0.3 a 0.7 las 3 curvas
 comienzan a decaer paulatinamente, lo que nos indica que el nivel de ruido
 hace que la red quede oscilando alrededor del atractor pero cada vez con
 un radio mayor.
\end_layout

\begin_layout Itemize
Luego, cuando el nivel de ruido esta entre 0.7 a 1 las 3 curvas tienen una
 pendiente pronunciada.
 Podemos pensar que en estos niveles de ruidos, la red esta tan perturbada
 que comienza a salirse de la cuenca del atractor, llegando a niveles de
 ruido mas altos, donde la red ya no reconoce mas nada y adopta valores
 aleatorios recorriendo todo el espacio de configuraciones.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename img/ejer2_024.jpg
	scale 50
	rotateOrigin center

\end_inset


\end_layout

\begin_layout Plain Layout

\shape italic
\size tiny
\begin_inset Caption

\begin_layout Plain Layout
Grado de reconocimiento <m> respecto del nivel de ruido para la relación
 
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}=0.024$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Aquí podemos observar un comportamiento de las redes con relación 
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}=0.024$
\end_inset

 muy similar al anterior donde la relación era 
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}=0.012$
\end_inset


\end_layout

\begin_layout Itemize
En comparación con el gráfico anterior, podemos destacar que la caída abrupta
 se produce para un nivel de ruido menor 
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}=0.012$
\end_inset

 la caída se produce con 
\begin_inset Formula $0.7\leq ruido\leq0.9$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}=0.024$
\end_inset

 la caída se produce con 
\begin_inset Formula $0.6\leq ruido\leq0.8$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename img/ejer2_036.jpg
	scale 50
	rotateOrigin center

\end_inset


\end_layout

\begin_layout Plain Layout

\shape italic
\size tiny
\begin_inset Caption

\begin_layout Plain Layout
Grado de reconocimiento <m> respecto del nivel de ruido para la relación
 
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}=0.036$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Nuevamente podemos ver en comparación con los gráficos anteriores que la
 caída abrupta se produce para un nivel de ruido menor
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}=0.012$
\end_inset

 la caída se produce con 
\begin_inset Formula $0.7\leq ruido\leq0.9$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}=0.024$
\end_inset

 la caída se produce con 
\begin_inset Formula $0.6\leq ruido\leq0.8$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}=0.036$
\end_inset

 la caída se produce con 
\begin_inset Formula $0.55\leq ruido\leq0.75$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
De la comparación entre los 3 gráficos podemos observar que:
\end_layout

\begin_layout Itemize
Para redes que conserven la misma relación 
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}$
\end_inset

, se comportan de manera muy similar ante distintos niveles de ruido.
\end_layout

\begin_layout Itemize
A medida que la relación 
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}$
\end_inset

 aumenta, el nivel de ruido al cual la red deja de reconocer es cada vez
 menor.
\end_layout

\begin_layout Standard
Este ultimo echo lo podemos observar en el siguiente gráfico donde comparamos
 el grado de reconocimiento de una red de 1664 neuronas para 3 cantidades
 de conceptos distintos, respecto del nivel de ruido.
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename img/ejer2_N1664.jpg
	scale 50
	rotateOrigin center

\end_inset


\end_layout

\begin_layout Plain Layout

\shape italic
\size tiny
\begin_inset Caption

\begin_layout Plain Layout
<m> para 3 cantidades de conceptos en una misma red, respecto del nivel
 de ruido.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Podemos notar claramente, como en una misma red neuronal, a medida que increment
amos la cantidad de conceptos, la misma deja de reconocer a niveles de ruido
 cada vez menores.
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Part*
Conclusiones
\end_layout

\begin_layout Subsection*
Redes deterministas
\end_layout

\begin_layout Enumerate
Pudimos ver claramente en los gráficos de las redes neuronales deterministas,
 que cuando 
\begin_inset Formula $0.138\leq\frac{N\text{º}conceptos}{N\text{º}neuronas}$
\end_inset

, la red deja de reconocer.
\end_layout

\begin_layout Enumerate
A medida que incrementamos el numero de neuronas en la red, los gráficos
 resultantes se asemejan a los vistos en la teoría.
\end_layout

\begin_layout Subsection*
Redes estocásticas
\end_layout

\begin_layout Enumerate
Redes que conservan la misma relación 
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}$
\end_inset

, se comportan de manera muy similar para cada valor de ruido.
\end_layout

\begin_layout Enumerate
A medida que aumenta el nivel de ruido, la red tiende a salir del atractor
 correcto, pasando por otros atractores.
 Para niveles de ruido muy alto, la red deja de comportarse como memoria
 asociativa y comienza a recorrer el espacio de configuraciones aleatoriamente.
\end_layout

\begin_layout Enumerate
A medida que aumenta relación 
\begin_inset Formula $\frac{N\text{º}conceptos}{N\text{º}neuronas}$
\end_inset

, el nivele de ruido al cual la red deja de reconocer es cada vez menor.
\end_layout

\end_body
\end_document
